{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-image\n",
    "from tensorflow.keras.layers import Conv2D, Conv2DTranspose, UpSampling2D\n",
    "from tensorflow.keras.layers import Activation, Dense, Dropout, Flatten, InputLayer\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "# from sklearn.model_selection import train_test_split\n",
    "from PIL import Image, ImageCms\n",
    "from skimage.color import rgb2lab, lab2rgb, rgb2gray\n",
    "from skimage.io import imsave\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# photos_path = './faces/Train/'\n",
    "# test_path = './faces/Test/'\n",
    "photos_path = './imagenet/pomegranate/train'\n",
    "test_path = './imagenet/pomegranate/test'\n",
    "# photos_path = './unsplash_photos/train'\n",
    "# test_path = './unsplash_photos/test'\n",
    "\n",
    "RGB = 'RGB'\n",
    "LAB = 'LAB'\n",
    "train_split = 0.8\n",
    "random_seed = 42\n",
    "# For faces and unsplash\n",
    "# resolution = (256,256)\n",
    "# For imagenet\n",
    "resolution = (64, 64)\n",
    "\n",
    "\n",
    "# Converter for Lab colourspace\n",
    "srgb_p = ImageCms.createProfile(\"sRGB\")\n",
    "lab_p  = ImageCms.createProfile(\"LAB\")\n",
    "\n",
    "rgb2lab_PIL = ImageCms.buildTransformFromOpenProfiles(srgb_p, lab_p, \"RGB\", \"LAB\")\n",
    "lab2rgb_PIL = ImageCms.buildTransformFromOpenProfiles(lab_p, srgb_p, \"LAB\", \"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos = []\n",
    "\n",
    "# using PIL\n",
    "# for photo in os.listdir(photos_path):\n",
    "#     rgb_photo = Image.open(os.path.join(photos_path, photo)).resize(resolution).convert(RGB)\n",
    "#     lab_photo =  ImageCms.applyTransform(rgb_photo, rgb2lab)\n",
    "#     lab_array = np.array(lab_photo)\n",
    "#     photos.append(lab_array)\n",
    "\n",
    "# Using Scikit Image\n",
    "for photo_name in os.listdir(photos_path):\n",
    "    photo = load_img(os.path.join(photos_path, photo_name))\n",
    "    photo = photo.resize(resolution)\n",
    "    photo_array = img_to_array(photo)\n",
    "    photos.append(photo_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "photos_np = np.array(photos, dtype= float)\n",
    "# photos_np /= 255.0\n",
    "\n",
    "split_len = int(len(photos_np) * train_split)\n",
    "\n",
    "photos_train = photos_np[:split_len]\n",
    "photos_train /= 255.0\n",
    "\n",
    "photos_val = photos_np[split_len:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(481, 64, 64, 3)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "photos_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "neurons_set_1 = [64, 128, 256, 512]\n",
    "neurons_set_2 = [256, 128]\n",
    "# neurons_set_2 = [256, 128, 64]\n",
    "upsampling_neurons = [128, 64, 32, 16]\n",
    "filter_size = (3,3)\n",
    "upsampling_filter_size = (2,2)\n",
    "activation_hidden = 'relu'\n",
    "activation_output = 'tanh'\n",
    "strides = 2\n",
    "\n",
    "epochs = 15\n",
    "steps_per_epoch = 1000\n",
    "\n",
    "# create model\n",
    "model = Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(InputLayer(input_shape= resolution + (1,)))\n",
    "\n",
    "# Hidden layer set 1\n",
    "for neuron in neurons_set_1:\n",
    "    model.add(Conv2D(neuron, filter_size, activation=activation_hidden, padding='same'))\n",
    "    model.add(Conv2D(neuron, filter_size, activation=activation_hidden, padding='same', strides=strides))\n",
    "    model.add(BatchNormalization())\n",
    "    \n",
    "# Hidden layer set 2\n",
    "for neuron in neurons_set_2:\n",
    "    model.add(Conv2D(neuron, filter_size, activation=activation_hidden, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "# Upsampling Hidden layer\n",
    "for neuron in upsampling_neurons:\n",
    "    model.add(UpSampling2D(upsampling_filter_size))\n",
    "    model.add(Conv2D(neuron, filter_size, activation=activation_hidden, padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "# prepare output layer\n",
    "model.add(Conv2D(2, (3,3), activation=activation_output, padding='same'))\n",
    "# model.add(UpSampling2D(upsampling_filter_size))\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss='mse')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_val = rgb2lab(photos_val/255.0)\n",
    "X_val = lab_val[:,:,:,0]\n",
    "X_val = X_val.reshape(X_val.shape + (1,))\n",
    "\n",
    "y_val = lab_val[:,:,:,1:]\n",
    "y_val /= 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.4470766912513042, 98.35009388875834, -9.190135242181551, 57.63323037027413)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = rgb2lab(photos_train[0])\n",
    "np.min(x[:,:,0]), np.max(x[:,:,0]), np.min(x[:,:,1:]), np.max(x[:,:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_56 (Conv2D)           (None, 64, 64, 64)        640       \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 8, 8, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "conv2d_63 (Conv2D)           (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 4, 4, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv2d_64 (Conv2D)           (None, 4, 4, 256)         1179904   \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_65 (Conv2D)           (None, 4, 4, 128)         295040    \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 4, 4, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_16 (UpSampling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_66 (Conv2D)           (None, 8, 8, 128)         147584    \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_17 (UpSampling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_67 (Conv2D)           (None, 16, 16, 64)        73792     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_18 (UpSampling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_68 (Conv2D)           (None, 32, 32, 32)        18464     \n",
      "_________________________________________________________________\n",
      "batch_normalization_44 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_19 (UpSampling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_69 (Conv2D)           (None, 64, 64, 16)        4624      \n",
      "_________________________________________________________________\n",
      "batch_normalization_45 (Batc (None, 64, 64, 16)        64        \n",
      "_________________________________________________________________\n",
      "conv2d_70 (Conv2D)           (None, 64, 64, 2)         290       \n",
      "=================================================================\n",
      "Total params: 6,410,258\n",
      "Trainable params: 6,407,090\n",
      "Non-trainable params: 3,168\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1000/1000 [==============================] - 218s 211ms/step - loss: 0.0153 - val_loss: 0.0239\n",
      "Epoch 2/15\n",
      "1000/1000 [==============================] - 218s 218ms/step - loss: 0.0060 - val_loss: 0.0238\n",
      "Epoch 3/15\n",
      "1000/1000 [==============================] - 220s 220ms/step - loss: 0.0050 - val_loss: 0.0230\n",
      "Epoch 4/15\n",
      "1000/1000 [==============================] - 246s 246ms/step - loss: 0.0044 - val_loss: 0.0215\n",
      "Epoch 5/15\n",
      "1000/1000 [==============================] - 254s 254ms/step - loss: 0.0041 - val_loss: 0.0327\n",
      "Epoch 6/15\n",
      "1000/1000 [==============================] - 190s 190ms/step - loss: 0.0038 - val_loss: 0.0241\n",
      "Epoch 7/15\n",
      "1000/1000 [==============================] - 189s 189ms/step - loss: 0.0037 - val_loss: 0.0233\n",
      "Epoch 8/15\n",
      "1000/1000 [==============================] - 185s 185ms/step - loss: 0.0035 - val_loss: 0.0232\n",
      "Epoch 9/15\n",
      "1000/1000 [==============================] - 184s 184ms/step - loss: 0.0033 - val_loss: 0.0218\n",
      "Epoch 10/15\n",
      "1000/1000 [==============================] - 185s 185ms/step - loss: 0.0031 - val_loss: 0.0223\n",
      "Epoch 11/15\n",
      "1000/1000 [==============================] - 184s 184ms/step - loss: 0.0030 - val_loss: 0.0215\n",
      "Epoch 12/15\n",
      "1000/1000 [==============================] - 186s 186ms/step - loss: 0.0029 - val_loss: 0.0223\n",
      "Epoch 13/15\n",
      "1000/1000 [==============================] - 185s 185ms/step - loss: 0.0029 - val_loss: 0.0219\n",
      "Epoch 14/15\n",
      "1000/1000 [==============================] - 185s 185ms/step - loss: 0.0028 - val_loss: 0.0207\n",
      "Epoch 15/15\n",
      "1000/1000 [==============================] - 184s 185ms/step - loss: 0.0027 - val_loss: 0.0219\n"
     ]
    }
   ],
   "source": [
    "# Image transformer\n",
    "datagen = ImageDataGenerator(\n",
    "    shear_range=0.2, zoom_range=0.2, rotation_range=20, horizontal_flip=True)\n",
    "\n",
    "# Generate training data\n",
    "batch_size = 64\n",
    "def batch_generator(batch_size):\n",
    "    for photos_batch in datagen.flow(photos_train, batch_size=batch_size):\n",
    "        # scikit-image\n",
    "        LAB_images = rgb2lab(photos_batch)\n",
    "        X_batch = LAB_images[:, :, :, 0]\n",
    "        Y_batch = LAB_images[:, :, :, 1:] / 128\n",
    "        yield (X_batch.reshape(X_batch.shape+(1,)), Y_batch)\n",
    "        # PIL\n",
    "        # X_batch = photos_batch[:,:,:,0]\n",
    "        # y_batch = photos_batch[:,:,:,1:]\n",
    "        # y_batch -= 128\n",
    "        # y_batch /= 128\n",
    "        # yield (X_batch.reshape(X_batch.shape + (1,)), y_batch)\n",
    "\n",
    "\n",
    "# Train model\n",
    "model_name = \"imagenet_pomeg_1000\"\n",
    "tensorboard = TensorBoard(log_dir=\"output/\"+model_name)\n",
    "checkpoint_filepath = './checkpoint/'+model_name\n",
    "model_checkpoint_callback = ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='loss',\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "history = model.fit(batch_generator(batch_size), callbacks=[tensorboard, model_checkpoint_callback], epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data= (X_val,y_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 1s 1s/step - loss: 0.0219\n",
      "0.021895894780755043\n"
     ]
    }
   ],
   "source": [
    "print(model.evaluate(X_val, y_val, batch_size=batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\ProgramData\\Anaconda3\\envs\\kewlar\\lib\\site-packages\\skimage\\_shared\\utils.py:394: UserWarning: Color data out of range: Z < 0 in 1 pixels\n",
      "  return func(*args, **kwargs)\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "d:\\ProgramData\\Anaconda3\\envs\\kewlar\\lib\\site-packages\\skimage\\_shared\\utils.py:394: UserWarning: Color data out of range: Z < 0 in 144 pixels\n",
      "  return func(*args, **kwargs)\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new directory is created!\n",
      "0 52 -5 0 255\n",
      "1 59 -36 0 255\n",
      "2 58 -23 0 255\n",
      "3 52 -8 0 255\n",
      "4 57 -12 0 255\n",
      "5 61 -12 0 255\n",
      "6 47 -23 0 255\n",
      "7 50 -11 0 255\n",
      "8 62 -24 0 237\n",
      "9 39 -28 0 255\n",
      "10 48 -7 0 255\n",
      "11 53 -26 0 255\n",
      "12 50 -7 0 255\n",
      "13 38 -16 0 255\n",
      "14 49 -11 0 255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n",
      "Lossy conversion from float64 to uint8. Range [0, 1]. Convert image to uint8 prior to saving to suppress this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 41 -9 0 255\n",
      "16 44 -18 0 255\n",
      "17 50 -7 0 255\n",
      "18 46 -7 0 255\n"
     ]
    }
   ],
   "source": [
    "testing_set = []\n",
    "\n",
    "# For PIL\n",
    "# for photo_name in os.listdir(test_path):\n",
    "#     rgb_photo = Image.open(os.path.join(test_path, photo_name)).resize(resolution).convert(RGB)\n",
    "#     lab_photo =  ImageCms.applyTransform(rgb_photo, rgb2lab)\n",
    "#     lab_array = np.array(lab_photo)\n",
    "#     testing_set.append(lab_array)\n",
    "\n",
    "# for Scikit image\n",
    "for photo_name in os.listdir(test_path):\n",
    "    photo = load_img(os.path.join(test_path, photo_name))\n",
    "    photo = photo.resize(resolution)\n",
    "    photo_array = img_to_array(photo)\n",
    "    testing_set.append(photo_array)\n",
    "\n",
    "testing_set = np.array(testing_set, dtype = float)\n",
    "lab_test = rgb2lab(testing_set/255.0)\n",
    "\n",
    "test_photos = lab_test[:,:,:,0]\n",
    "test_photos = test_photos.reshape(test_photos.shape + (1,))\n",
    "\n",
    "output = model.predict(test_photos)\n",
    "output *= 128\n",
    "output = output.astype(int)\n",
    "\n",
    "output_path = f\"Result/imagenet_pomeg_{epochs}_{steps_per_epoch}/\"\n",
    "\n",
    "# Check whether the specified path exists or not\n",
    "exists = os.path.exists(output_path)\n",
    "if not exists:\n",
    "   # Create a new directory because it does not exist\n",
    "   os.makedirs(output_path)\n",
    "   print(\"The new directory is created!\")\n",
    "   \n",
    "testing_set = testing_set.astype(int)\n",
    "for i in range(len(output)):\n",
    "   canvas = np.zeros(resolution + (3,))\n",
    "   bw_part = testing_set[i][:,:,0]\n",
    "   canvas[:,:,0] = bw_part\n",
    "   canvas[:,:,1:] = output[i]\n",
    "   print(i, np.max(output[i]),np.min(output[i]), np.min(testing_set[i]), np.max(testing_set[i]))\n",
    "   rgb_canvas = lab2rgb(canvas)\n",
    "     \n",
    "   # using PIL\n",
    "   #  lab_image = Image.fromarray(canvas, mode=\"LAB\")\n",
    "   #  rgb_image = ImageCms.applyTransform(lab_image, lab2rgb)\n",
    "   #  rgb_image.save(output_path+f\"output_{i}.jpeg\")\n",
    "\n",
    "   # using scikit image\n",
    "   imsave(output_path+f\"output_{i}.jpeg\", lab2rgb(canvas))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "model_json = model.to_json()\n",
    "with open(f\"{model_name}_{epochs}.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(f\"{model_name}_{epochs}.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('kewlar')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aed6f3b4749d3eb63f8c4756e0ac52957cbd5283609569251b1431ba96b2e7f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
